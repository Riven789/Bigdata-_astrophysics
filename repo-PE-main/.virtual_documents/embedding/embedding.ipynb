


import torch.nn as nn
import torch


class SpectrogramEmbedder(nn.Module):
    """
    Embedding network for spectrograms.
    """
    def __init__(self, embedding_dim=128):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.flatten = nn.Flatten()
        height, width = 128, 128
        self.fc = nn.Linear(64 * height * width, embedding_dim)

    def forward(self, x):
        x = self.cnn(x)
        x = self.flatten(x)
        x = self.fc(x)
        return x






class JointSpectrogramEmbedder(nn.Module):
    """
    Joint embedding network for h1 and l1 spectrograms.
    """
    def __init__(self, embedder):
        super().__init__()
        self.embedder = embedder

    def forward(self, h1, l1):
        h1_embedding = self.embedder(h1)
        l1_embedding = self.embedder(l1)
        return torch.cat((h1_embedding, l1_embedding), dim=-1)
