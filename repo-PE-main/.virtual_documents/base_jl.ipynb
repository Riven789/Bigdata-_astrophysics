import numpy as np
import torch
import h5py
import os

from dataset import Spectrogram

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import Dataset, DataLoader





num_files = 1

data_dir = '/Users/shreyaggarwal/Desktop/Project8581/data/train/'
files = [h5py.File(data_dir+file, 'r') for file in os.listdir(data_dir) if file[:11] == 'background-']
param_keys = ['chirp_mass', 'mass_ratio', 'distance', 'psi', 'phi', 'dec', 'snr']

dataset = []
theta = []

for f in files[:num_files]:

    print(f)

    batch = np.array([f['H1'], f['L1']])
    params = np.array([np.array(f[key]) for key in param_keys[:-1]])

    spec = Spectrogram(num_points=batch.shape[-1])

    data, params = spec.forward(batch, params.T)

    dataset.append(data)
    theta.append(params)

dataset = torch.cat(dataset)
theta = torch.cat(theta)







class DataGenerator(Dataset):
    def __len__(self):
        return dataset.shape[0]

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        # return theta and data
        return (
            theta[idx].to(device=device),
            dataset[idx].to(device=device))

torch_dataset = DataGenerator()

train_set_size = 0.8
val_set_size = 0.1
test_set_size = 0.1

train_data, val_data, test_data = torch.utils.data.random_split(
    torch_dataset, [train_set_size, val_set_size, test_set_size])

TRAIN_BATCH_SIZE = 30
VAL_BATCH_SIZE = 30

train_data_loader = DataLoader(
    train_data, batch_size=TRAIN_BATCH_SIZE,
    shuffle=True
)

val_data_loader = DataLoader(
    val_data, batch_size=VAL_BATCH_SIZE,
    shuffle=True
)

test_data_loader = DataLoader(
    test_data, batch_size=1,
    shuffle=False
)








# Simple 2D Conv Encoder (you can replace with ResNet or other architectures)
class Encoder(nn.Module):
    def __init__(self, embedding_dim):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),  # input: (B,1,H,W)
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4))     # preserve more spatial info
        )
        self.fc = nn.Sequential(
            nn.Flatten(),                   # now input to FC is (B, 64*4*4)
            nn.Linear(64*4*4, embedding_dim)
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)
        return x





import torch
import torch.nn.functional as F
from torch import Tensor


class VICRegLoss(torch.nn.Module):
    """Implementation of the VICReg loss [0].

    This implementation is based on the code published by the authors [1].

    - [0] VICReg, 2022, https://arxiv.org/abs/2105.04906
    - [1] https://github.com/facebookresearch/vicreg/

    Attributes:
        lambda_param:
            Scaling coefficient for the invariance term of the loss.
        mu_param:
            Scaling coefficient for the variance term of the loss.
        nu_param:
            Scaling coefficient for the covariance term of the loss.
        eps:
            Epsilon for numerical stability.

    """

    def __init__(
        self,
        lambda_param: float = 25.0,
        mu_param: float = 25.0,
        nu_param: float = 1.0,
        eps: float = 0.0001,
        max_std: float = 1.0,
    ):
        """Initializes the VICRegLoss module with the specified parameters."""
        super(VICRegLoss, self).__init__()

        self.lambda_param = lambda_param
        self.mu_param = mu_param
        self.nu_param = nu_param
        self.max_std = max_std
        self.eps = eps

    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:
        """Returns VICReg loss.

        Args:
            z_a:
                Tensor with shape (batch_size, ..., dim).
            z_b:
                Tensor with shape (batch_size, ..., dim).

        Returns:
            The computed VICReg loss.

        """

        # Invariance term of the loss
        inv_loss = invariance_loss(x=z_a, y=z_b)

        # Variance and covariance terms of the loss
        var_loss = 0.5 * (
            variance_loss(x=z_a, eps=self.eps, max_std=self.max_std)
            + variance_loss(x=z_b, eps=self.eps, max_std=self.max_std)
        )
        cov_loss = covariance_loss(x=z_a) + covariance_loss(x=z_b)

        # Total VICReg loss
        loss = (
            self.lambda_param * inv_loss
            + self.mu_param * var_loss
            + self.nu_param * cov_loss
        )
        return loss, (inv_loss, var_loss, cov_loss)


def invariance_loss(x: Tensor, y: Tensor) -> Tensor:
    """Returns VICReg invariance loss.

    Args:
        x:
            Tensor with shape (batch_size, ..., dim).
        y:
            Tensor with shape (batch_size, ..., dim).

    Returns:
        The computed VICReg invariance loss.
    """
    return F.mse_loss(x, y)


def variance_loss(
    x: Tensor, eps: float = 0.0001, max_std: float = 1.0
) -> Tensor:
    """Returns VICReg variance loss.

    Args:
        x:
            Tensor with shape (batch_size, ..., dim).
        eps:
            Epsilon for numerical stability.

    Returns:
        The computed VICReg variance loss.
    """
    std = torch.sqrt(x.var(dim=0) + eps)
    loss = torch.mean(F.relu(max_std - std))
    return loss


def covariance_loss(x: Tensor) -> Tensor:
    """Returns VICReg covariance loss.

    Generalized version of the covariance loss with support for tensors with more than
    two dimensions. Adapted from VICRegL:
    https://github.com/facebookresearch/VICRegL/blob/803ae4c8cd1649a820f03afb4793763e95317620/main_vicregl.py#L299

    Args:
        x: Tensor with shape (batch_size, ..., dim).

    Returns:
          The computed VICReg covariance loss.
    """  # noqa
    x = x - x.mean(dim=0)
    batch_size = x.size(0)
    dim = x.size(-1)
    # nondiag_mask has shape (dim, dim) with 1s on all non-diagonal entries.
    nondiag_mask = ~torch.eye(dim, device=x.device, dtype=torch.bool)

    # cov has shape (..., dim, dim)
    cov = torch.einsum("b...c,b...d->...cd", x, x) / (batch_size - 1)

    loss = cov[..., nondiag_mask].pow(2).sum(-1) / dim
    return loss.mean()





len(train_data_loader)


import tqdm

num_epochs = 10
embedding_dim = 128
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Models
#encoder1 = Encoder(embedding_dim).to(device)
#encoder2 = Encoder(embedding_dim).to(device)

encoder1 = Encoder(embedding_dim)
encoder2 = Encoder(embedding_dim)
#encoder1.apply(init_weights)


# Loss and optimizer
criterion = VICRegLoss()
optimizer = optim.Adam(list(encoder1.parameters()) + list(encoder2.parameters()), lr=1e-3)

train_losses = []
val_losses = []


i = 0

len_train = len(train_data_loader)

for epoch in range(num_epochs):

    print(epoch)
    encoder1.train()
    encoder2.train()
    total_train_loss = 0
    total_val_loss = 0

    # --- Training ---
    for (param_data, batch_data), z in zip(train_data_loader, tqdm.tqdm(range(len_train))):
        batch_data = batch_data.to(device)
        param_data = param_data.to(device)  # optional: not used in VICReg

        # Split channels
        ch1 = batch_data[:, :, 0, :, :].unsqueeze(2)  # (B, 10, 1, H, W)
        ch2 = batch_data[:, :, 1, :, :].unsqueeze(2)

        
        # Flatten time dimension
        b, t, c, h, w = ch1.shape
        ch1 = ch1.view(b*t, c, h, w)
        ch2 = ch2.view(b*t, c, h, w)

        

        emb1 = encoder1(ch1).view(b, t, -1)  # (B, T, D)
        emb2 = encoder2(ch2).view(b, t, -1)

        
        # VICReg loss across time shifts
        loss1 = sum(criterion(emb1[:, i, :], emb1[:, i+1, :])[0] for i in range(t-1))
        loss2 = sum(criterion(emb2[:, i, :], emb2[:, i+1, :])[0] for i in range(t-1))
        loss = (loss1 + loss2) / (2 * (t-1))
        total_train_loss += loss.item()
        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


    # --- Validation ---
    encoder1.eval()
    encoder2.eval()
    
    with torch.no_grad():
        for param, batch in val_data_loader:
            batch = batch.to(device)

            ch1 = batch[:, :, 0, :, :].unsqueeze(2)
            ch2 = batch[:, :, 1, :, :].unsqueeze(2)

            b, t, c, h, w = ch1.shape
            ch1 = ch1.view(b*t, c, h, w)
            ch2 = ch2.view(b*t, c, h, w)

            emb1 = encoder1(ch1)
            emb2 = encoder2(ch2)

            emb1 = emb1.view(b, t, -1)
            emb2 = emb2.view(b, t, -1)

            loss1 = 0
            loss2 = 0
            for i in range(t-1):
                loss1 += criterion(emb1[:, i, :], emb1[:, i+1, :])[0]
                loss2 += criterion(emb2[:, i, :], emb2[:, i+1, :])[0]

            loss = (loss1 + loss2) / (2 * (t-1))
            total_val_loss += loss.item()


    avg_train_loss = total_train_loss / len(train_data_loader)
    avg_val_loss = total_val_loss / len(val_data_loader)

    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)

    print(f"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.plot(train_losses, label='Training Loss')
#plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.yscale('log')
plt.legend()
plt.grid()
plt.savefig('sine_wave.png')
plt.show()


# Save the final pretrained encoders
torch.save({
    'encoder1_state_dict': encoder1.state_dict(),
    'encoder2_state_dict': encoder2.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, f='emb.pth')




